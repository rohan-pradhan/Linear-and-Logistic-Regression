{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from statistics import mean \n",
    "with np.load(\"notMNIST.npz\") as data :\n",
    "    Data, Target = data [\"images\"], data[\"labels\"]\n",
    "    posClass = 2\n",
    "    negClass = 9\n",
    "    dataIndx = (Target==posClass) + (Target==negClass)\n",
    "    Data = Data[dataIndx]/255.\n",
    "    Target = Target[dataIndx].reshape(-1, 1)\n",
    "    Target[Target==posClass] = 1\n",
    "    Target[Target==negClass] = 0\n",
    "    np.random.seed(521)\n",
    "    randIndx = np.arange(len(Data))\n",
    "    np.random.shuffle(randIndx)\n",
    "    Data, Target = Data[randIndx], Target[randIndx]\n",
    "    trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "    validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "    testData, testTarget = Data[3600:], Target[3600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start_train(n_iterations,learning_rate,fix_decay_regularizer,isAdam,batch_size):\n",
    "    \n",
    "    m, n, k = trainData.shape\n",
    "    n_epoch = int(n_iterations*batch_size/m)\n",
    "    \n",
    "    #Data Handler\n",
    "    valid_accuracy_log = np.zeros(n_epoch)\n",
    "    valid_entropy= np.zeros(n_epoch)\n",
    "    train_accuracy_log = np.zeros(n_epoch)\n",
    "    train_entropy = np.zeros(n_epoch)\n",
    "    \n",
    "    #Test Data (|concatenate a 28x28 training data point to 1x(784))\n",
    "    x_in = tf.placeholder(tf.float32,[None,n*k], name=\"dataset_in\")\n",
    "    y_in = tf.placeholder(tf.float32, [None,1], name=\"true_value\")\n",
    "    \n",
    "    #Validation Data\n",
    "    valid_data = tf.constant(validData.reshape(-1,n*k),dtype=tf.float32, name=\"validation_data\")\n",
    "    valid_label = tf.constant(validTarget,dtype=tf.float32, name=\"validation_label\")\n",
    "    \n",
    "    #Weights\n",
    "    w = tf.Variable(tf.zeros([n*k, 1], dtype=np.float32), name=\"weight\")\n",
    "    b = tf.Variable(tf.zeros([1], dtype=np.float32), name=\"bias\")\n",
    "    \n",
    "    if fix_decay_regularizer:\n",
    "        decaybias = 0\n",
    "    else:\n",
    "        decaybias = 0.01 #tf.Variable(0.01, name=\"decay_regularizer\")\n",
    "    \n",
    "    #optimizer\n",
    "    if isAdam:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    #train Loss functions with SGD\n",
    "    y_pred = tf.add(tf.matmul(x_in,w),b)\n",
    "    loss_weight_decay=tf.nn.l2_loss(w) #0 anyways %loss_weight_decay=h*tf.square(tf.norm(w,ord=2)) #0 anyways\n",
    "    train_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits= y_pred,labels = y_in)\n",
    "    train_loss_total = tf.reduce_mean(tf.add(train_cross_entropy,tf.multiply(decaybias,loss_weight_decay)))\n",
    "    \n",
    "    #Validation Loss functions\n",
    "    valid_prediction = (tf.add(tf.matmul(valid_data,w), b))\n",
    "    valid_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits= valid_prediction,labels = valid_label)\n",
    "    valid_loss_total = tf.reduce_mean(tf.add(valid_cross_entropy,tf.multiply(decaybias,loss_weight_decay)))\n",
    "    \n",
    "    #Validation and Data Loggers\n",
    "    #since we are a probability, if greater than 50%, set to 1 else set to 0 (thats why we round)\n",
    "    _, train_acc_log = tf.metrics.accuracy(labels=y_in, predictions=tf.round(tf.sigmoid(y_pred)))\n",
    "    _, valid_acc_log = tf.metrics.accuracy(labels=valid_label, predictions=tf.round(tf.sigmoid(valid_prediction)))\n",
    "    \n",
    "    #Train\n",
    "    train = optimizer.minimize(train_loss_total)\n",
    "    \n",
    "    #initialized all vars\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #run!\n",
    "    with tf.Session() as sess:\n",
    "        #Local variables for accuracy metric\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(init)\n",
    "        for epoch in range(n_epoch):\n",
    "            #clear our history (we use this to calculate the average accuracy/loss for each iteration)\n",
    "            entropy_history = 0\n",
    "            accuracy_history = 0\n",
    "            #reshuffles the dataset in unison. for each epoch\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(trainData)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(trainTarget)\n",
    "            #every 100 epoch, update the user\n",
    "            #batches of batch_size up to total number of datapoints (3500)\n",
    "            for offset in range(0, 3500, batch_size):\n",
    "                #find the proper dataset\n",
    "                batch_x = trainData[offset:offset+batch_size]\n",
    "                batch_y = trainTarget[offset:offset+batch_size]\n",
    "                #train\n",
    "                sess.run((train),feed_dict={x_in:batch_x.reshape(-1,n*k),y_in:batch_y})\n",
    "                #add to history\n",
    "                entropy_history = entropy_history + sess.run(train_loss_total,feed_dict={x_in:batch_x.reshape(-1,n*k),y_in:batch_y})\n",
    "                accuracy_history= accuracy_history + sess.run(train_acc_log,feed_dict={x_in:batch_x.reshape(-1,n*k),y_in:batch_y})\n",
    "    \n",
    "            #calculate and store the average history for that iteration\n",
    "            train_entropy[epoch] = entropy_history/(3500/batch_size)\n",
    "            train_accuracy_log[epoch] = accuracy_history/(3500/batch_size)\n",
    "            #store validation data\n",
    "            valid_accuracy_log[epoch] = sess.run(valid_acc_log)\n",
    "            valid_entropy[epoch] = sess.run(valid_loss_total)\n",
    "        #evaluate test data\n",
    "        best_weight = w.eval()\n",
    "        best_bias = b.eval()\n",
    "        test_accuracy = start_test(best_weight,best_bias)\n",
    "        return train_entropy,train_accuracy_log,valid_accuracy_log,valid_entropy,test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FOR PART 1\n",
    "#for storing data\n",
    "train_acc_log = np.zeros(len(trainData))\n",
    "valid_acc_log = np.zeros(len(trainData))\n",
    "valid_loss_log = np.zeros(len(trainData))\n",
    "train_loss = np.zeros(len(trainData))\n",
    "\n",
    "isAdam = False\n",
    "#for averaging\n",
    "#train_acc_avrg=0\n",
    "#train_loss_avrg=0\n",
    "##valid_acc_avrg=0\n",
    "#valid_loss_avrg=0\n",
    "#test_acc_avrg = 0\n",
    "\n",
    "#m=2\n",
    "#for i in range(m):\n",
    "train_loss,train_acc_log,valid_acc_log,valid_loss_log,test_acc_log= start_train(5000,0.001,False,isAdam,500) \n",
    "    \n",
    "#    train_acc_avrg = train_acc_avrg + mean(train_acc_log)\n",
    "#    train_loss_avrg = train_loss_avrg + mean(train_loss)\n",
    "#    valid_acc_avrg = valid_acc_avrg + mean(valid_acc_log)\n",
    "#    valid_loss_avrg = valid_loss_avrg + mean(valid_loss_log)\n",
    "#    test_acc_avrg = test_acc_avrg + test_acc_log\n",
    "\n",
    "#train_acc_avrg = train_acc_avrg/m\n",
    "#train_loss_avrg = train_loss_avrg/m\n",
    "#valid_acc_avrg = valid_acc_avrg/m\n",
    "#valid_loss_avrg = valid_loss_avrg/m\n",
    "#test_acc_avrg=test_acc_avrg/m\n",
    "\n",
    "#i just plot the most recent data. not the average. the average was used to find the end data\n",
    "import matplotlib.pyplot as p \n",
    "p.plot(train_loss,'b-', label=\"train\")\n",
    "p.plot(train_acc_log,'b-')\n",
    "p.plot(valid_acc_log,'r-',label =\"validation\")\n",
    "p.plot(valid_loss_log,'r-')\n",
    "p.legend(numpoints = 1)\n",
    "p.show()\n",
    "print(\"test_acc=\",test_acc_log)\n",
    "print(\"train_accuracy=\",np.max(train_acc_log),\"validation_accuracy=\",np.max(valid_acc_log),\"train_loss function=\",np.min(train_loss))\n",
    "#avrg_min_loss,avrg_max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for learning 0.005\n",
    "#test_acc= 0.97931033\n",
    "#train_accuracy= 0.9803337710244315 validation_accuracy= 0.979593813419342 train_loss function= 0.06626365067703384\n",
    "\n",
    "#for learning 0.001\n",
    "#test_acc= 0.9655172\n",
    "#train_accuracy= 0.9767628141811916 validation_accuracy= 0.9800000190734863 train_loss function= 0.07979274221829005\n",
    "\n",
    "#for learning rate 0.0001\n",
    "#test_acc= 0.9724138\n",
    "#train_accuracy= 0.9713712079184396 validation_accuracy= 0.9800000190734863 train_loss function= 0.16093909527574266\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FOR PART 2\n",
    "#for storing data\n",
    "SGD_loss = np.zeros(len(trainData))\n",
    "Adam_loss_log = np.zeros(len(trainData))\n",
    "\n",
    "isAdam = False\n",
    "SGD_loss,_,_,_,_= start_train(5000,0.001,False,isAdam,500) \n",
    "isAdam = True \n",
    "Adam_loss_log,_,_,_,_= start_train(5000,0.001,False,isAdam,500) \n",
    "\n",
    "import matplotlib.pyplot as p \n",
    "p.plot(SGD_loss,'b-', label=\"train_SGD\")\n",
    "p.plot(Adam_loss_log,'r-',label =\"train_Adam\")\n",
    "p.legend(numpoints = 1)\n",
    "p.show()\n",
    "print(\"Adam_loss=\",np.max(Adam_loss_log),\"SGD_Loss=\",np.max(SGD_loss))\n",
    "#avrg_min_loss,avrg_max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
