{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "with np.load(\"notMNIST.npz\") as data :\n",
    "    Data, Target = data [\"images\"], data[\"labels\"]\n",
    "    posClass = 2\n",
    "    negClass = 9\n",
    "    dataIndx = (Target==posClass) + (Target==negClass)\n",
    "    Data = Data[dataIndx]/255.\n",
    "    Target = Target[dataIndx].reshape(-1, 1)\n",
    "    Target[Target==posClass] = 1\n",
    "    Target[Target==negClass] = 0\n",
    "    np.random.seed(521)\n",
    "    randIndx = np.arange(len(Data))\n",
    "    np.random.shuffle(randIndx)\n",
    "    Data, Target = Data[randIndx], Target[randIndx]\n",
    "    trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "    validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "    testData, testTarget = Data[3600:], Target[3600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_train(n_iterations,learning_rate,fix_decay_regularizer,batch_size):\n",
    "    \n",
    "    m, n, k = trainData.shape\n",
    "    n_epoch = int(n_iterations*batch_size/m)\n",
    "    \n",
    "    #Data Handler\n",
    "    accuracy_log = np.zeros(n_epoch)\n",
    "    loss = np.zeros(n_epoch)\n",
    "\n",
    "    #Data (concatenate a 28x28 training data point to 1x(784))\n",
    "    x_in = tf.placeholder(tf.float32,[None,n*k], name=\"dataset_in\")\n",
    "    y_in = tf.placeholder(tf.float32, [None,1], name=\"true_value\")\n",
    "\n",
    "    #Weights\n",
    "    w = tf.Variable(tf.zeros([n*k, 1], dtype=np.float32), name=\"weight\")\n",
    "    b = tf.Variable(tf.zeros([1], dtype=np.float32), name=\"bias\")\n",
    "    \n",
    "    if fix_decay_regularizer:\n",
    "        decaybias = 0\n",
    "    else:\n",
    "        decaybias = 0.01 #tf.Variable(0.01, name=\"decay_regularizer\")\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "                \n",
    "    #Loss functions\n",
    "    y_pred = tf.add(tf.matmul(x_in,w),b)\n",
    "    loss_weight_decay=tf.nn.l2_loss(w) #0 anyways %loss_weight_decay=h*tf.square(tf.norm(w,ord=2)) #0 anyways\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits= y_pred,labels = y_in)\n",
    "    loss_total = tf.reduce_mean(tf.add(cross_entropy,tf.multiply(decaybias,loss_weight_decay)))\n",
    "    \n",
    "    #Data Loggers\n",
    "    #since we are a probability, if greater than 50%, set to 1 else set to 0 (thats why we round)\n",
    "    _, acc_log = tf.metrics.accuracy(labels=y_in, predictions=tf.round(tf.sigmoid(y_pred)))\n",
    "        \n",
    "    #Train\n",
    "    train = optimizer.minimize(loss_total)\n",
    "    \n",
    "    #initialized all vars\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #run!\n",
    "    with tf.Session() as sess:\n",
    "        #Local variables for accuracy metric\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(init)\n",
    "        for epoch in range(n_epoch):\n",
    "            #reshuffles the dataset in unison. for each epoch\n",
    "            #Code concept obtained from https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(trainData)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(trainTarget)\n",
    "            #every 100 epoch, update the user\n",
    "            #batches of batch_size up to total number of datapoints (3500)\n",
    "            for offset in range(0, 3500, batch_size):\n",
    "                batch_x = trainData[offset:offset+batch_size]\n",
    "                batch_y = trainTarget[offset:offset+batch_size]\n",
    "                _,loss[epoch],accuracy_log[epoch]=sess.run((train,loss_total,acc_log),feed_dict={x_in:batch_x.reshape(-1,n*k),y_in:batch_y})\n",
    "                \n",
    "        return loss,accuracy_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_log = np.zeros(len(trainData))\n",
    "loss = np.zeros(len(trainData))\n",
    "avrg_min_loss = 0\n",
    "avrg_max_acc = 0\n",
    "for i in range(100):\n",
    "    loss,acc_log= start_train(5000,0.0001,False,500) \n",
    "    avrg_min_loss = np.min(loss) + avrg_min_loss\n",
    "    avrg_max_acc = np.max(acc_log) + avrg_max_acc\n",
    "    \n",
    "avrg_min_loss = avrg_min_loss/100\n",
    "avrg_max_acc = avrg_max_acc/100\n",
    "import matplotlib.pyplot as p \n",
    "p.plot(loss,'b-', label=\"train\")\n",
    "p.plot(acc_log,'b-')\n",
    "p.legend(numpoints = 1)\n",
    "p.show()\n",
    "avrg_min_loss,avrg_max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(0.036388177424669266, 0.9802048802375793) for learning rate 0.005\n",
    "(0.05242612585425377, 0.9766522645950317) for learning 0.001\n",
    "(0.14226588606834412, 0.9712700843811035) for learning rate 0.0001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_func_test(n_iterations,learning_rate,fix_decay_regularizer,batch_size):\n",
    "    \n",
    "    m, n, k = trainData.shape\n",
    "    n_epoch = int(n_iterations*batch_size/m)\n",
    "    \n",
    "    accuracy_log = np.zeros(n_epoch)\n",
    "    loss = np.zeros(n_epoch)\n",
    "\n",
    "    #Data (concatenate a 28x28 training data point to 1x(784))\n",
    "    x_in = tf.constant(trainData.reshape(-1,n*k),dtype=tf.float32, name=\"dataset_in\")\n",
    "    y_in = tf.constant(trainTarget,dtype=tf.float32, name=\"true_value\")\n",
    "        \n",
    "    #Weights\n",
    "    w = tf.Variable(tf.zeros([n*k, 1], dtype=np.float32), name=\"weight\")\n",
    "    b = tf.Variable(tf.zeros([1], dtype=np.float32), name=\"bias\")\n",
    "    \n",
    "    if fix_decay_regularizer:\n",
    "        decaybias = 0\n",
    "    else:\n",
    "        decaybias = 0.01 #tf.Variable(0.01, name=\"decay_regularizer\")\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "                \n",
    "    #Loss functions\n",
    "    y_pred = tf.add(tf.matmul(x_in,w),b)\n",
    "    loss_weight_decay=tf.nn.l2_loss(w) #0 anyways %loss_weight_decay=h*tf.square(tf.norm(w,ord=2)) #0 anyways\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits= y_pred,labels = y_in)\n",
    "    loss_total = tf.reduce_mean(tf.add(cross_entropy,tf.multiply(decaybias,loss_weight_decay)))\n",
    "    \n",
    "    #Data Loggers\n",
    "    acc, acc_log = tf.metrics.accuracy(labels=y_in, predictions=tf.round(tf.sigmoid(y_pred)))\n",
    "        \n",
    "    #Train\n",
    "    train = optimizer.minimize(loss_total)\n",
    "    \n",
    "    #initialized all vars\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #run!\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(init)\n",
    "        for epoch in range(n_epoch):\n",
    "                #every 100 epoch, update the user\n",
    "            if epoch % 1000 == 0:\n",
    "                print(\"epoch=\",epoch,\"accuracy=\",sess.run(acc_log), \"loss=\",loss_total.eval())\n",
    "            sess.run(train)\n",
    "            accuracy_log[epoch] = sess.run(acc_log)\n",
    "            loss[epoch] = sess.run(loss_total)\n",
    "        return loss,accuracy_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_log = np.zeros(len(trainData))\n",
    "loss = np.zeros(len(trainData))\n",
    "loss,acc_log= loss_func_test(3500,0.005,False,3500) \n",
    "import matplotlib.pyplot as p \n",
    "p.plot(loss,'b-', label=\"loss\")\n",
    "p.plot(acc_log,'r-', label=\"accuracy\")\n",
    "p.legend(numpoints = 1)\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
