{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "n_samples = 3500\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "INPUT_DIMENSIONS = 784\n",
    "\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "\n",
    "def get_batch(size, data, target): \n",
    "    index = np.arange(0, len(data))\n",
    "    np.random.shuffle(index)\n",
    "    index = index[:size]\n",
    "    batch_data =  [data[ i] for i in index]\n",
    "    batch_target = [target[ i] for i in index]\n",
    "    return np.asarray(batch_data), np.asarray(batch_target)\n",
    "\n",
    "def shuffle(data, target):\n",
    "    index = np.arange(0, len(data))\n",
    "    np.random.shuffle(index)\n",
    "    shuffled_data = [data[i] for i in index]\n",
    "    shuffled_target = [target[i] for i in index]\n",
    "    return np.asarray(shuffled_data), np.asarray(shuffled_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_run_linear_reg(num_epochs, batch_size, p_learning_rate, decay_coefficient, test_set):\n",
    "    epochs = num_epochs\n",
    "    BATCH_SIZE = batch_size \n",
    "    learning_rate = p_learning_rate\n",
    "    beta = decay_coefficient\n",
    "    train_accuracy_log = np.zeros(epochs+1)\n",
    "\n",
    "    \n",
    "    with np.load(\"notMNIST.npz\") as data :\n",
    "        Data, Target = data [\"images\"], data[\"labels\"]\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(521)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "#         beta = 0.0001\n",
    "\n",
    "\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "\n",
    "        trainData_Flat, trainTarget_Flat = trainData.reshape(trainData.shape[0],-1), trainTarget\n",
    "        validData_Flat, validTarget_Flat = validData.reshape(validData.shape[0],-1), validTarget\n",
    "        testData_Flat, testTarget_Flat = testData.reshape(testData.shape[0],-1), testTarget\n",
    "\n",
    "        X = tf.placeholder(tf.float32, [None, 784] )\n",
    "        X_pred = tf.placeholder(tf.float32, [None, 784] )\n",
    "        Y = tf.placeholder(tf.float32, [None, 1])\n",
    "        W = tf.Variable(tf.zeros([784, 1]))\n",
    "        B = tf.Variable(tf.zeros([1]))\n",
    "        regularizer = tf.nn.l2_loss(W)\n",
    "\n",
    "\n",
    "        pred = tf.add(tf.matmul(X,W), B)\n",
    "        valid_prediction = tf.round(tf.add(tf.matmul(X,W), B))\n",
    "#         valid_prediction = tf.round(tf.nn.sigmoid(tf.add(tf.matmul(X,W), B)))\n",
    "\n",
    "       #For training accuracy logging\n",
    "        _, train_acc_log = tf.metrics.accuracy(labels=Y, predictions=valid_prediction,name=\"training_accuracy\")\n",
    "\n",
    "        cost = tf.reduce_sum(((tf.pow(pred-Y, 2))/(2*n_samples))+(beta*regularizer))\n",
    "    #     cost = tf.reduce_mean(cost + beta * regularizer)\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "        with tf.Session() as sess: \n",
    "\n",
    "            def calc(x, y):\n",
    "                predictions = tf.round(tf.nn.sigmoid(tf.add(tf.matmul(x,W), B)))\n",
    "                error = tf.reduce_mean(tf.square(y - predictions))\n",
    "                return [predictions, error]\n",
    "            sess.run(init_local)\n",
    "            sess.run(init)\n",
    "        \n",
    "\n",
    "            train_count = len(trainData_Flat)    \n",
    "            cost_list = []\n",
    "            \n",
    "            for i in range(1, epochs + 1):\n",
    "                #clear our history (we use this to calculate the average accuracy/loss for each iteration)\n",
    "                accuracy_history = 0                \n",
    "                cost_final = np.empty(shape=[1],dtype=float)\n",
    "                for start, end in zip(range(0, train_count, BATCH_SIZE),\n",
    "                                      range(BATCH_SIZE, train_count + 1,BATCH_SIZE)):\n",
    "                    sess.run(optimizer, feed_dict={X: trainData_Flat[start:end],\n",
    "                                               Y: trainTarget_Flat[start:end]})\n",
    "                    #add to history\n",
    "                    accuracy_history= accuracy_history + sess.run(train_acc_log,feed_dict={X: trainData_Flat[start:end],Y: trainTarget_Flat[start:end]})\n",
    " \n",
    "                train_accuracy_log[i] = accuracy_history/(3500/BATCH_SIZE)\n",
    "\n",
    "\n",
    "                trainData_Flat, trainTarget_Flat = shuffle(trainData_Flat, trainTarget_Flat)\n",
    "                \n",
    "                cost_list.append(sess.run(cost, feed_dict={X:trainData_Flat, Y:trainTarget_Flat}))\n",
    "\n",
    "            if (test_set==0):\n",
    "                predictions = sess.run(valid_prediction, feed_dict={X: validData_Flat})\n",
    "                acc, acc_op = tf.metrics.accuracy(labels=validTarget_Flat, \n",
    "                                       predictions=predictions)\n",
    "            \n",
    "            elif (test_set==1): \n",
    "                predictions = sess.run(valid_prediction, feed_dict={X: testData_Flat})\n",
    "                acc, acc_op = tf.metrics.accuracy(labels=testTarget_Flat, \n",
    "                                      predictions=predictions)       \n",
    "            else:\n",
    "                predictions = sess.run(valid_prediction, feed_dict={X: trainData_Flat})\n",
    "                acc, acc_op = tf.metrics.accuracy(labels=trainTarget_Flat, \n",
    "                                      predictions=predictions)\n",
    "\n",
    "            init_l = tf.local_variables_initializer()\n",
    "            sess.run(init_l)\n",
    "\n",
    "            print(\"_,accuracy\",sess.run([acc, acc_op]))\n",
    "            print(sess.run([acc]))\n",
    "            \n",
    "            \n",
    "            # Output\n",
    "            #[0.0, 0.66666669]\n",
    "            #[0.66666669]\n",
    "\n",
    "#             save_path = saver.save(sess, \"./linear_reg_model.ckpt\")\n",
    "#             print(\"Model saved in path: %s\" % save_path)\n",
    "            return cost_list, sess.run([acc]),train_accuracy_log\n",
    "\n",
    "\n",
    "def q1a(): \n",
    "    learning_rate_1, _, _ = train_and_run_linear_reg(2857,500,0.005,0, False)\n",
    "    learning_rate_2, _, _ = train_and_run_linear_reg(2857,500,0.001,0, False)\n",
    "    learning_rate_3, _, _ = train_and_run_linear_reg(2857,500,0.0001,0, False)\n",
    "    \n",
    "    epochs = list(range(1,2858))\n",
    "    \n",
    "    plt.plot(epochs, learning_rate_1, label=\"Learning rate = 0.005\")\n",
    "    plt.plot(epochs, learning_rate_2, label=\"Learning rate = 0.001\")\n",
    "    plt.plot(epochs, learning_rate_3, label=\"Learning rate = 0.0001\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Training Loss Function vs. Number of Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Converged Loss Value: \")\n",
    "    print(\"0.005 learning rate: \", learning_rate_1[-1])\n",
    "    print(\"0.001 learning rate: \", learning_rate_2[-1])\n",
    "    print(\"0.0001 learning rate: \", learning_rate_3[-1])\n",
    "    \n",
    "def q1b():\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    batch_size_1, _, _ = train_and_run_linear_reg(2857,500,0.005,0, False)\n",
    "    end = time.time()\n",
    "    batch_time_1 = (end-start)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    batch_size_2, _, _ = train_and_run_linear_reg(8571,1500,0.005,0, False)\n",
    "    end = time.time()\n",
    "    batch_time_2 = (end-start)\n",
    "    \n",
    "    start = time.time()\n",
    "    batch_size_3, _, _ = train_and_run_linear_reg(20000,3500,0.005,0, False)\n",
    "    end = time.time()\n",
    "    batch_time_3 = (end-start)\n",
    "    \n",
    "    print(\"Time to train and predict: \")\n",
    "    print(\"Batch size of 500: \", batch_time_1)\n",
    "    print(\"Batch size of 1500: \", batch_time_2)\n",
    "    print(\"Batch size of 3500: \", batch_time_3)\n",
    "    \n",
    "    print (\"------\")\n",
    "    \n",
    "    print(\"Final MSE: \")\n",
    "    print(\"Batch size of 500: \", batch_size_1[-1])\n",
    "    print(\"Batch size of 1500: \", batch_size_2[-1])\n",
    "    print(\"Batch size of 3500: \", batch_size_3[-1])\n",
    "    \n",
    "def q1c_partA():\n",
    "    \n",
    "    weight_decay = [0.,0.001,0.1,1]\n",
    "    \n",
    "    _, accuracy_weight_decay_0, _ = train_and_run_linear_reg(2857,500,0.005, weight_decay[0], False)\n",
    "    \n",
    "    _, accuracy_weight_decay_1, _ = train_and_run_linear_reg(2857,500,0.005, weight_decay[1], False)\n",
    "    \n",
    "    _, accuracy_weight_decay_2, _ = train_and_run_linear_reg(2857,500,0.005, weight_decay[2], False)\n",
    "    \n",
    "    _, accuracy_weight_decay_3, _ = train_and_run_linear_reg(2857,500,0.005, weight_decay[3], False)\n",
    "    \n",
    "    print (\"-------\")\n",
    "    \n",
    "    print(\"Validation Accuracies for different weight decay values: \")\n",
    "    print(weight_decay[0], \": \", accuracy_weight_decay_0 )\n",
    "    print(weight_decay[1], \": \", accuracy_weight_decay_1 )\n",
    "    print(weight_decay[2], \": \", accuracy_weight_decay_2 )\n",
    "    print(weight_decay[3], \": \", accuracy_weight_decay_3 )\n",
    "    \n",
    "    \n",
    "def q1c_partB():\n",
    "    _, accuracy = train_and_run_linear_reg(2857,500,0.005, 0, True)\n",
    "    print(\"Test set Accuracy: \", accuracy)\n",
    "    \n",
    "\n",
    "#q1a()\n",
    "#q1b()\n",
    "#q1c_partA()\n",
    "#q1c_partB()\n",
    "\n",
    "def qlc_Q2Part3():\n",
    "    #Train set\n",
    "    _, _, _ = train_and_run_linear_reg(2857,500,0.001,0, 0)\n",
    "    #validation set\n",
    "    mse_loss_train, _ , _= train_and_run_linear_reg(2857,500,0.001,0, 1)\n",
    "    #test set\n",
    "    _, _ ,_= train_and_run_linear_reg(2857,500,0.001,0, 2)\n",
    "    epochs = list(range(1,2858))\n",
    "    plt.plot(epochs, mse_loss_train, label=\"Learning rate = 0.001,Train MSE\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Training Loss Function vs. Number of Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()    \n",
    "\n",
    "qlc_Q2Part3()\n",
    "# train_and_run_linear_reg(2857,500,0.005,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, target):\n",
    "    if (not(isinstance(predictions, np.ndarray)) or not(isinstance(target, np.ndarray))):\n",
    "        raise TypeError (\"Input to function is not numpy.ndarray\")\n",
    "        \n",
    "    if (len(predictions) != len(target)):\n",
    "        raise ValueError(\"Prediction and Target shapes are not equal\")\n",
    "#     print (target)\n",
    "#     predictions = np.ndarray.tolist(predictions)\n",
    "#     target = np.ndarray.tolist(target)\n",
    "    \n",
    "#     print (predictions)\n",
    "#     print (len(target))\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for i,_ in np.ndenumerate(target):\n",
    "        if (target[i] == predictions[i]):\n",
    "            counter=counter+1\n",
    "       \n",
    "           \n",
    "    return (counter/len(predictions))\n",
    "\n",
    "\n",
    "def closed_form_linear_regression():\n",
    "     import time\n",
    "     with np.load(\"notMNIST.npz\") as data :\n",
    "        Data, Target = data [\"images\"], data[\"labels\"]\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(521)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "#         beta = 0.0001\n",
    "\n",
    "\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "\n",
    "        trainData_Flat, trainTarget_Flat = trainData.reshape(trainData.shape[0],-1), trainTarget\n",
    "        trainData_Flat_withBias = np.hstack([np.ones((len(trainData_Flat), 1)), trainData_Flat])\n",
    "        \n",
    "        validData_Flat, validTarget_Flat = validData.reshape(validData.shape[0],-1), validTarget\n",
    "        validData_Flat_withBias = np.hstack([np.ones((len(validData_Flat), 1)), validData_Flat])\n",
    "        \n",
    "        testData_Flat, testTarget_Flat = testData.reshape(testData.shape[0],-1), testTarget\n",
    "        testData_Flat_withBias = np.hstack([np.ones((len(testData_Flat), 1)), testData_Flat])        \n",
    "        \n",
    "        \n",
    "        X = tf.placeholder(tf.float32, [None, INPUT_DIMENSIONS+1] )\n",
    "        Y = tf.placeholder(tf.float32, [None, 1])\n",
    "        pred = tf.placeholder(tf.float32, [None, 1])\n",
    "#         mse = lambda pred, lab: ((pred-np.array(lab))**2).sum()/DS_SIZE\n",
    "        \n",
    "        mse = tf.reduce_sum(((tf.pow(pred-Y, 2))/(2*n_samples)))\n",
    "        \n",
    "        weights_optimizer = tf.matrix_solve_ls(X, Y, 0, fast=True)\n",
    "#         cost = tf.reduce_sum(((tf.pow(pred-Y, 2))/(2*n_samples))+(beta*regularizer))\n",
    "        \n",
    "        with tf.Session() as s:\n",
    "            tf.initialize_all_variables().run()\n",
    "            start = time.time()\n",
    "            params = s.run(weights_optimizer, feed_dict={X:trainData_Flat_withBias, Y:trainTarget_Flat})\n",
    "            end = time.time()\n",
    "\n",
    "            print (\"Time to train: \", end-start)\n",
    "            weights = params\n",
    "#             print(\"Solution for parameters:\\n\",type(PARAMETERS_NORMEQ))\n",
    "            \n",
    "            lambda_predictions = lambda x: np.absolute(np.round(weights.T.dot(x)))\n",
    "        \n",
    "            train_predictions = np.array([lambda_predictions(x) for x in trainData_Flat_withBias])\n",
    "            print(\"MSE: \", s.run(mse, feed_dict={pred:train_predictions, Y:trainTarget_Flat}))\n",
    "            \n",
    "            \n",
    "            \n",
    "            predictions_validation = np.array([lambda_predictions(x) for x in validData_Flat_withBias])\n",
    "            print(\"Validation accuracy: \", calculate_accuracy(predictions_validation, validTarget_Flat))\n",
    "            \n",
    "            predictions_test = np.array([lambda_predictions(x) for x in testData_Flat_withBias])\n",
    "\n",
    "            print(\"Test accuracy: \", calculate_accuracy(predictions_test, testTarget))\n",
    "            \n",
    " \n",
    "                \n",
    "            \n",
    "           \n",
    "        \n",
    "        \n",
    "closed_form_linear_regression()        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For part 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from statistics import mean \n",
    "with np.load(\"notMNIST.npz\") as data :\n",
    "    Data, Target = data [\"images\"], data[\"labels\"]\n",
    "    posClass = 2\n",
    "    negClass = 9\n",
    "    dataIndx = (Target==posClass) + (Target==negClass)\n",
    "    Data = Data[dataIndx]/255.\n",
    "    Target = Target[dataIndx].reshape(-1, 1)\n",
    "    Target[Target==posClass] = 1\n",
    "    Target[Target==negClass] = 0\n",
    "    np.random.seed(521)\n",
    "    randIndx = np.arange(len(Data))\n",
    "    np.random.shuffle(randIndx)\n",
    "    Data, Target = Data[randIndx], Target[randIndx]\n",
    "    trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "    validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "    testData, testTarget = Data[3600:], Target[3600:]\n",
    "    \n",
    "def start_test(weights,bias):\n",
    "    m, n, k = trainData.shape\n",
    "\n",
    "    #Validation Data\n",
    "    test_data = tf.constant(testData.reshape(-1,n*k),dtype=tf.float32, name=\"test_data\")\n",
    "    test_label = tf.constant(testTarget,dtype=tf.float32, name=\"test_label\")\n",
    "\n",
    "    test_prediction = (tf.add(tf.matmul(test_data,weights), bias))\n",
    "\n",
    "    _, test_acc_log = tf.metrics.accuracy(labels=test_label, predictions=tf.round(tf.sigmoid(test_prediction)))\n",
    "\n",
    "    #initialized all vars\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #run!\n",
    "    with tf.Session() as sess:\n",
    "        #Local variables for accuracy metric\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(init)        \n",
    "        return sess.run(test_acc_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_train(n_iterations,learning_rate,fix_decay_regularizer,isAdam,batch_size):\n",
    "    \n",
    "    m, n, k = trainData.shape\n",
    "    n_epoch = int(n_iterations*batch_size/m)\n",
    "    \n",
    "    #Data Handler\n",
    "    valid_accuracy_log = np.zeros(n_epoch)\n",
    "    valid_entropy= np.zeros(n_epoch)\n",
    "    train_accuracy_log = np.zeros(n_epoch)\n",
    "    train_entropy = np.zeros(n_epoch)\n",
    "    \n",
    "    #Test Data (|concatenate a 28x28 training data point to 1x(784))\n",
    "    x_in = tf.placeholder(tf.float32,[None,n*k], name=\"dataset_in\")\n",
    "    y_in = tf.placeholder(tf.float32, [None,1], name=\"true_value\")\n",
    "    \n",
    "    #Validation Data\n",
    "    valid_data = tf.constant(validData.reshape(-1,n*k),dtype=tf.float32, name=\"validation_data\")\n",
    "    valid_label = tf.constant(validTarget,dtype=tf.float32, name=\"validation_label\")\n",
    "    \n",
    "    #Weights\n",
    "    w = tf.Variable(tf.zeros([n*k, 1], dtype=np.float32), name=\"weight\")\n",
    "    b = tf.Variable(tf.zeros([1], dtype=np.float32), name=\"bias\")\n",
    "    \n",
    "    if fix_decay_regularizer:\n",
    "        decaybias = 0\n",
    "    else:\n",
    "        decaybias = 0.01 #tf.Variable(0.01, name=\"decay_regularizer\")\n",
    "    \n",
    "    #optimizer\n",
    "    if isAdam:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    #train Loss functions with SGD\n",
    "    y_pred = tf.add(tf.matmul(x_in,w),b)\n",
    "    loss_weight_decay=decaybias*tf.nn.l2_loss(w) #0 anyways %loss_weight_decay=h*tf.square(tf.norm(w,ord=2)) #0 anyways\n",
    "    train_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits= y_pred,labels = y_in)\n",
    "    train_loss_total = tf.reduce_mean(tf.add(train_cross_entropy,loss_weight_decay))\n",
    "    \n",
    "    #Validation Loss functions\n",
    "    valid_prediction = (tf.add(tf.matmul(valid_data,w), b))\n",
    "    valid_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits= valid_prediction,labels = valid_label)\n",
    "    valid_loss_total = tf.reduce_mean(tf.add(valid_cross_entropy,loss_weight_decay))\n",
    "    \n",
    "    #Validation and Data Loggers\n",
    "    #since we are a probability, if greater than 50%, set to 1 else set to 0 (thats why we round)\n",
    "    _, train_acc_log = tf.metrics.accuracy(labels=y_in, predictions=tf.round(tf.sigmoid(y_pred)))\n",
    "    _, valid_acc_log = tf.metrics.accuracy(labels=valid_label, predictions=tf.round(tf.sigmoid(valid_prediction)))\n",
    "    \n",
    "    #Train\n",
    "    train = optimizer.minimize(train_loss_total)\n",
    "    \n",
    "    #initialized all vars\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #run!\n",
    "    with tf.Session() as sess:\n",
    "        #Local variables for accuracy metric\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(init)\n",
    "        for epoch in range(n_epoch):\n",
    "            #clear our history (we use this to calculate the average accuracy/loss for each iteration)\n",
    "            entropy_history = 0\n",
    "            accuracy_history = 0\n",
    "            #reshuffles the dataset in unison. for each epoch\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(trainData)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(trainTarget)\n",
    "            #batches of batch_size up to total number of datapoints (3500)\n",
    "            for offset in range(0, 3500, batch_size):\n",
    "                #find the proper dataset\n",
    "                batch_x = trainData[offset:offset+batch_size]\n",
    "                batch_y = trainTarget[offset:offset+batch_size]\n",
    "                #train\n",
    "                sess.run((train),feed_dict={x_in:batch_x.reshape(-1,n*k),y_in:batch_y})\n",
    "                #add to history\n",
    "                entropy_history = entropy_history + sess.run(train_loss_total,feed_dict={x_in:batch_x.reshape(-1,n*k),y_in:batch_y})\n",
    "                accuracy_history= accuracy_history + sess.run(train_acc_log,feed_dict={x_in:batch_x.reshape(-1,n*k),y_in:batch_y})\n",
    "    \n",
    "            #calculate and store the average history for that iteration\n",
    "            train_entropy[epoch] = entropy_history/(3500/batch_size)\n",
    "            train_accuracy_log[epoch] = accuracy_history/(3500/batch_size)\n",
    "            #store validation data\n",
    "            valid_accuracy_log[epoch] = sess.run(valid_acc_log)\n",
    "            valid_entropy[epoch] = sess.run(valid_loss_total)\n",
    "        #evaluate test data\n",
    "        best_weight = w.eval()\n",
    "        best_bias = b.eval()\n",
    "        test_accuracy = start_test(best_weight,best_bias)\n",
    "        return train_entropy,train_accuracy_log,valid_accuracy_log,valid_entropy,test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FOR PART 1\n",
    "#for storing data\n",
    "train_acc_log = np.zeros(len(trainData))\n",
    "valid_acc_log = np.zeros(len(trainData))\n",
    "valid_loss_log = np.zeros(len(trainData))\n",
    "train_loss = np.zeros(len(trainData))\n",
    "\n",
    "isAdam = False\n",
    "train_loss,train_acc_log,valid_acc_log,valid_loss_log,test_acc_log= start_train(5000,0.001,True,isAdam,500) \n",
    "    \n",
    "import matplotlib.pyplot as p \n",
    "p.plot(train_loss,'b-', label=\"train\")\n",
    "p.plot(train_acc_log,'b-')\n",
    "p.plot(valid_acc_log,'r-',label =\"validation\")\n",
    "p.plot(valid_loss_log,'r-')\n",
    "p.legend(numpoints = 1)\n",
    "p.show()\n",
    "print(\"test_acc=\",test_acc_log)\n",
    "print(\"train_accuracy=\",np.max(train_acc_log),\"validation_accuracy=\",np.max(valid_acc_log),\"train_loss function=\",np.min(train_loss))\n",
    "#avrg_min_loss,avrg_max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FOR PART 2\n",
    "#for storing data\n",
    "\n",
    "isAdam = False\n",
    "SGD_loss,_,_,_,_= start_train(5000,0.001,False,isAdam,500) \n",
    "isAdam = True \n",
    "Adam_loss_log,_,_,_,_= start_train(5000,0.001,False,isAdam,500) \n",
    "\n",
    "import matplotlib.pyplot as p \n",
    "p.plot(SGD_loss,'b-', label=\"train_SGD\")\n",
    "p.plot(Adam_loss_log,'r-',label =\"train_Adam\")\n",
    "p.legend(numpoints = 1)\n",
    "p.show()\n",
    "print(\"Adam_loss=\",np.max(Adam_loss_log),\"SGD_Loss=\",np.max(SGD_loss))\n",
    "#avrg_min_loss,avrg_max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For Part 3\n",
    "\n",
    "isAdam = True \n",
    "Adam_loss_log,Adam_acc_log,_,_,_= start_train(5000,0.001,True,isAdam,500) \n",
    "mse_loss_train, _,mse_acc_log = train_and_run_linear_reg(714,500,0.001,0, 1)\n",
    "\n",
    "import matplotlib.pyplot as p \n",
    "p.plot(Adam_acc_log,'m-', label=\"Adam Accuracy\")\n",
    "p.plot(mse_acc_log,'c-', label=\"Linear Accuracy\")\n",
    "p.legend(numpoints = 1)\n",
    "p.show() \n",
    "p.plot(Adam_loss_log,'r-',label =\"Adam Train\")\n",
    "p.plot(mse_loss_train,'b-',label=\"Linear Train\")\n",
    "p.legend(numpoints = 1)\n",
    "p.show()\n",
    "print(\"Adam_loss=\",np.max(Adam_loss_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
